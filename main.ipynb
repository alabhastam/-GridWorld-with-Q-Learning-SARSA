{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aabdollahii/gridworld-with-q-learning-sarsa?scriptVersionId=263559824\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"7fdde1fe","metadata":{"papermill":{"duration":0.003794,"end_time":"2025-09-23T14:03:03.139559","exception":false,"start_time":"2025-09-23T14:03:03.135765","status":"completed"},"tags":[]},"source":["<div style=\"background-color:#0d1117; color:#c9d1d9; padding:20px; border-radius:8px; font-family:Segoe UI, sans-serif; line-height:1.6;\">\n","  <h1 style=\"color:#58a6ff;\">üèÅ GridWorld: Q-Learning vs SARSA</h1>\n","  \n","  <p>\n","    In this project, we explore two cornerstone algorithms of <strong>Classic Reinforcement Learning</strong>:\n","    <span style=\"color:#d2a8ff;\">Q-Learning</span> and <span style=\"color:#d2a8ff;\">SARSA</span>.\n","    Using a custom-built <strong>GridWorld</strong> environment, we train an agent to navigate from a start\n","    position to a goal while avoiding traps and obstacles.\n","  </p>\n","\n","  <h2 style=\"color:#58a6ff;\">üîç Why GridWorld?</h2>\n","  <p>\n","    GridWorld provides a clean, interpretable environment where we can visualize learning policies \n","    and compare algorithm behaviors side-by-side. Adding traps and movement penalties \n","    forces each algorithm to balance risk and reward ‚Äî a great setup to see their differences in action.\n","  </p>\n","\n","  <h2 style=\"color:#58a6ff;\">üìà Project Goals</h2>\n","  <ul>\n","    <li>Implement a minimal but functional GridWorld without external dependencies.</li>\n","    <li>Train agents using <strong>Q-Learning</strong> (off-policy) and <strong>SARSA</strong> (on-policy).</li>\n","    <li>Track and compare learning curves, convergence speeds, and final policies.</li>\n","    <li>Visualize results in a dark-themed, publication-ready format.</li>\n","  </ul>\n","\n","  <h2 style=\"color:#58a6ff;\">üß† Key Difference</h2>\n","  <p>\n","    <strong>Q-Learning</strong> always updates toward the <em>best possible action</em> in the next state, \n","    even if the agent doesn't take it.<br>\n","    <strong>SARSA</strong> updates based on the <em>actual action</em> taken in the next state.\n","    This subtle change can lead to dramatically different navigation styles.\n","  </p>\n","\n","  <p style=\"margin-top:15px; font-style:italic; color:#8b949e;\">\n","    By the end of this notebook, you'll see how two seemingly similar algorithms \n","    can make very different choices in the same world.\n","  </p>\n","</div>\n"]},{"cell_type":"markdown","id":"53442ee1","metadata":{"papermill":{"duration":0.00257,"end_time":"2025-09-23T14:03:03.145243","exception":false,"start_time":"2025-09-23T14:03:03.142673","status":"completed"},"tags":[]},"source":["<div style=\"background-color:#0d1117; color:#c9d1d9; padding:18px; border-radius:8px; font-family:Segoe UI, sans-serif; line-height:1.6;\">\n","  <h2 style=\"color:#58a6ff;\">üõ† Step 1 ‚Äî Setup & Imports</h2>\n","  <p>\n","    In this step, we prepare the core components needed for our <strong>GridWorld</strong> project:\n","  </p>\n","  <ul>\n","    <li>Importing <code>numpy</code> and <code>matplotlib</code> for numerical operations and visualizations.</li>\n","    <li>Configuring <strong>dark-themed plots</strong> for a cleaner Kaggle output.</li>\n","    <li>Setting a <strong>random seed</strong> for reproducibility across runs.</li>\n","    <li>Defining the <strong>environment grid size</strong> and available <strong>agent actions</strong>.</li>\n","  </ul>\n","  <p>\n","    This minimal setup ensures our environment is ready for defining GridWorld mechanics \n","    and plugging in <span style=\"color:#d2a8ff;\">Q-Learning</span> and <span style=\"color:#d2a8ff;\">SARSA</span> algorithms in the next steps.\n","  </p>\n","</div>\n"]},{"cell_type":"code","execution_count":1,"id":"de7854f4","metadata":{"execution":{"iopub.execute_input":"2025-09-23T14:03:03.15172Z","iopub.status.busy":"2025-09-23T14:03:03.151388Z","iopub.status.idle":"2025-09-23T14:03:03.162424Z","shell.execute_reply":"2025-09-23T14:03:03.161419Z"},"papermill":{"duration":0.016532,"end_time":"2025-09-23T14:03:03.164332","exception":false,"start_time":"2025-09-23T14:03:03.1478","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Environment: 6x6 | Actions: ['U', 'R', 'D', 'L']\n"]}],"source":["\n","# Step 1 ‚Äî Setup & Imports\n","\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import random\n","\n","# Optional: make plots dark-themed by default for Kaggle\n","plt.style.use('dark_background')\n","\n","# For reproducibility\n","RANDOM_SEED = 42\n","random.seed(RANDOM_SEED)\n","np.random.seed(RANDOM_SEED)\n","\n","# Environment size\n","GRID_ROWS = 6\n","GRID_COLS = 6\n","\n","# Actions (Up, Right, Down, Left)\n","ACTIONS = ['U', 'R', 'D', 'L']\n","ACTION_IDX = {a: i for i, a in enumerate(ACTIONS)}\n","\n","print(f\"Environment: {GRID_ROWS}x{GRID_COLS} | Actions: {ACTIONS}\")\n"]},{"cell_type":"markdown","id":"7c3ee792","metadata":{"papermill":{"duration":0.002537,"end_time":"2025-09-23T14:03:03.169765","exception":false,"start_time":"2025-09-23T14:03:03.167228","status":"completed"},"tags":[]},"source":["<div style=\"background-color:#0d1117; color:#c9d1d9; padding:18px; border-radius:8px; font-family:Segoe UI, sans-serif; line-height:1.6;\">\n","  <h2 style=\"color:#58a6ff;\">üåç Step 2 ‚Äî Building the GridWorld Environment</h2>\n","  <p>\n","    Here we define our custom <strong>GridWorld</strong> ‚Äî a rectangular grid where an agent\n","    navigates from a <span style=\"color:#d2a8ff;\">start position</span> to a <span style=\"color:#d2a8ff;\">goal</span>,\n","    avoiding obstacles and traps along the way.\n","  </p>\n","  <h3 style=\"color:#58a6ff;\">Key Features</h3>\n","  <ul>\n","    <li>Supports <strong>obstacles (#)</strong> that block movement.</li>\n","    <li>Includes <strong>traps (X)</strong> that end the episode with a penalty.</li>\n","    <li>Grants a reward of <code>+10</code> for reaching the goal (<strong>G</strong>).</li>\n","    <li>Applies a penalty of <code>-5</code> for traps, and <code>-1</code> for each step to encourage efficiency.</li>\n","    <li>Maintains agent‚Äôs state as a <code>(row, col)</code> coordinate.</li>\n","  </ul>\n","  <p>\n","    The <code>step()</code> method handles movement, boundaries, obstacles, rewards, and terminal states.\n","    The <code>render()</code> method lets us <strong>visualize the grid</strong> and monitor the agent‚Äôs position.\n","  </p>\n","</div>\n"]},{"cell_type":"code","execution_count":2,"id":"0af7c2af","metadata":{"execution":{"iopub.execute_input":"2025-09-23T14:03:03.176647Z","iopub.status.busy":"2025-09-23T14:03:03.17629Z","iopub.status.idle":"2025-09-23T14:03:03.187133Z","shell.execute_reply":"2025-09-23T14:03:03.186325Z"},"papermill":{"duration":0.016152,"end_time":"2025-09-23T14:03:03.18868","exception":false,"start_time":"2025-09-23T14:03:03.172528","status":"completed"},"tags":[]},"outputs":[],"source":["# Step 2 ‚Äî Define GridWorld Environment\n","\n","class GridWorld:\n","    def __init__(self, rows, cols, start, goal, obstacles=None, traps=None):\n","        self.rows = rows\n","        self.cols = cols\n","        self.start = start\n","        self.goal = goal\n","        self.obstacles = obstacles if obstacles else []\n","        self.traps = traps if traps else []\n","        self.reset()\n","    \n","    def reset(self):\n","        \"\"\"Reset the agent to the start position.\"\"\"\n","        self.agent_pos = self.start\n","        return self.state()\n","    \n","    def state(self):\n","        \"\"\"Return current state as (row, col) tuple.\"\"\"\n","        return self.agent_pos\n","    \n","    def step(self, action):\n","        \"\"\"Take an action and return new_state, reward, done.\"\"\"\n","        r, c = self.agent_pos\n","        if action == 'U': r -= 1\n","        elif action == 'D': r += 1\n","        elif action == 'L': c -= 1\n","        elif action == 'R': c += 1\n","        \n","        # Stay inside bounds\n","        if r < 0: r = 0\n","        if r >= self.rows: r = self.rows - 1\n","        if c < 0: c = 0\n","        if c >= self.cols: c = self.cols - 1\n","        \n","        # If next cell is obstacle ‚Üí stay put\n","        if (r, c) in self.obstacles:\n","            r, c = self.agent_pos\n","        \n","        self.agent_pos = (r, c)\n","        \n","        # Determine reward and done\n","        if self.agent_pos == self.goal:\n","            return self.state(), 10, True  # Goal reward\n","        elif self.agent_pos in self.traps:\n","            return self.state(), -5, True  # Trap penalty\n","        else:\n","            return self.state(), -1, False  # Step cost\n","    \n","    def render(self):\n","        \"\"\"Print the current grid with agent position.\"\"\"\n","        grid = [['.' for _ in range(self.cols)] for _ in range(self.rows)]\n","        for (ro, co) in self.obstacles:\n","            grid[ro][co] = '#'\n","        for (rt, ct) in self.traps:\n","            grid[rt][ct] = 'X'\n","        gr, gc = self.goal\n","        grid[gr][gc] = 'G'\n","        ar, ac = self.agent_pos\n","        grid[ar][ac] = 'A'\n","        \n","        for row in grid:\n","            print(' '.join(row))\n","        print()\n"]},{"cell_type":"code","execution_count":3,"id":"722b05f7","metadata":{"execution":{"iopub.execute_input":"2025-09-23T14:03:03.196041Z","iopub.status.busy":"2025-09-23T14:03:03.195709Z","iopub.status.idle":"2025-09-23T14:03:03.201772Z","shell.execute_reply":"2025-09-23T14:03:03.200782Z"},"papermill":{"duration":0.011826,"end_time":"2025-09-23T14:03:03.203359","exception":false,"start_time":"2025-09-23T14:03:03.191533","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["A . . . . .\n",". # . . X .\n",". . . # . .\n",". . . # . .\n",". # . . X .\n",". . . . . G\n","\n"]}],"source":["# Create a 6x6 GridWorld example\n","env = GridWorld(\n","    rows=GRID_ROWS,\n","    cols=GRID_COLS,\n","    start=(0, 0),\n","    goal=(5, 5),\n","    obstacles=[(1, 1), (2, 3), (3, 3), (4, 1)],\n","    traps=[(1, 4), (4, 4)]\n",")\n","\n","state = env.reset()\n","env.render()\n"]},{"cell_type":"markdown","id":"9b54fac7","metadata":{"papermill":{"duration":0.002599,"end_time":"2025-09-23T14:03:03.208962","exception":false,"start_time":"2025-09-23T14:03:03.206363","status":"completed"},"tags":[]},"source":["<div style=\"background-color:#0d1117; color:#c9d1d9; padding:18px; border-radius:8px; font-family:Segoe UI, sans-serif; line-height:1.6;\">\n","  <h2 style=\"color:#58a6ff;\">ü§ñ Step 3 ‚Äî Implementing Q-Learning</h2>\n","  <p>\n","    In this step, we apply <strong>Q-Learning</strong>, an <em>off-policy</em> reinforcement learning algorithm,\n","    to train our agent in the GridWorld environment.\n","  </p>\n","  <h3 style=\"color:#58a6ff;\">Workflow</h3>\n","  <ol>\n","    <li><strong>Initialize the Q-table</strong> ‚Äî one value for each (state, action) pair.</li>\n","    <li><strong>Choose actions</strong> with an Œµ-greedy strategy: \n","        explore randomly with probability <code>Œµ</code> or exploit the best-known action.</li>\n","    <li><strong>Update the Q-value</strong> with the formula:\n","      <pre style=\"background-color:#161b22; padding:6px; border-radius:4px; color:#d2a8ff;\">\n","Q(s,a) ‚Üê Q(s,a) + Œ± [ r + Œ≥ max(Q(s‚Ä≤,a‚Ä≤)) ‚àí Q(s,a) ]\n","      </pre>\n","    </li>\n","    <li>Repeat until the agent consistently reaches the goal efficiently.</li>\n","  </ol>\n","  <p>\n","    Because Q-Learning is off-policy, it always updates toward the <strong>best possible next action</strong> ‚Äî\n","    even if our Œµ-greedy agent chooses a different path in practice.\n","  </p>\n","</div>\n"]},{"cell_type":"code","execution_count":4,"id":"c6b7cf73","metadata":{"execution":{"iopub.execute_input":"2025-09-23T14:03:03.215962Z","iopub.status.busy":"2025-09-23T14:03:03.215625Z","iopub.status.idle":"2025-09-23T14:03:03.283707Z","shell.execute_reply":"2025-09-23T14:03:03.28253Z"},"papermill":{"duration":0.074099,"end_time":"2025-09-23T14:03:03.285854","exception":false,"start_time":"2025-09-23T14:03:03.211755","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Training complete! Final Q-table shape: (36, 4)\n"]}],"source":["# Step 3 ‚Äî Q-Learning Implementation\n","\n","def epsilon_greedy(Q, state, epsilon):\n","    \"\"\"Choose action index using epsilon-greedy policy.\"\"\"\n","    if random.uniform(0, 1) < epsilon:\n","        return random.randint(0, len(ACTIONS)-1)  # Explore\n","    else:\n","        return np.argmax(Q[state])  # Exploit\n","\n","def train_q_learning(env, episodes=500, alpha=0.1, gamma=0.99, epsilon=0.1):\n","    # Initialize Q-table: rows*cols states x actions\n","    Q = np.zeros((env.rows * env.cols, len(ACTIONS)))\n","    episode_rewards = []\n","\n","    for ep in range(episodes):\n","        state = env.reset()\n","        total_reward = 0\n","        done = False\n","\n","        while not done:\n","            s_idx = state[0] * env.cols + state[1]  # Flatten (row,col) ‚Üí index\n","            action_idx = epsilon_greedy(Q, s_idx, epsilon)\n","            action = ACTIONS[action_idx]\n","\n","            new_state, reward, done = env.step(action)\n","            ns_idx = new_state[0] * env.cols + new_state[1]\n","\n","            # Q-learning update rule\n","            Q[s_idx, action_idx] += alpha * (reward + gamma * np.max(Q[ns_idx]) - Q[s_idx, action_idx])\n","\n","            state = new_state\n","            total_reward += reward\n","\n","        episode_rewards.append(total_reward)\n","    \n","    return Q, episode_rewards\n","\n","# --- Train Q-Learning agent ---\n","Q_table, q_rewards = train_q_learning(env, episodes=300, alpha=0.1, gamma=0.95, epsilon=0.2)\n","print(\"Training complete! Final Q-table shape:\", Q_table.shape)\n"]},{"cell_type":"markdown","id":"ddf298d2","metadata":{"papermill":{"duration":0.003906,"end_time":"2025-09-23T14:03:03.294067","exception":false,"start_time":"2025-09-23T14:03:03.290161","status":"completed"},"tags":[]},"source":["<div style=\"background-color:#0d1117; color:#c9d1d9; padding:18px; border-radius:8px; font-family:Segoe UI, sans-serif; line-height:1.6;\">\n","  <h2 style=\"color:#58a6ff;\">üß≠ Step 4-Implementing SARSA</h2>\n","  <p>\n","    Here we apply <strong>SARSA</strong>, an <em>on-policy</em> reinforcement learning algorithm.\n","    Unlike Q-Learning, SARSA updates its Q-values based on the <strong>actual action</strong> the agent \n","    takes in the next state ‚Äî not the optimal action in theory.\n","  </p>\n","  <h3 style=\"color:#58a6ff;\">Workflow</h3>\n","  <ol>\n","    <li>Initialize the Q-table for all (state, action) pairs.</li>\n","    <li>Select an action using the Œµ-greedy policy.</li>\n","    <li>After executing the action, select the <em>next action</em> from the next state \n","        <strong>using the same policy</strong>.</li>\n","    <li>Update the Q-value with:\n","      <pre style=\"background-color:#161b22; padding:6px; border-radius:4px; color:#d2a8ff;\">\n","Q(s,a) ‚Üê Q(s,a) + Œ± [ r + Œ≥ Q(s‚Ä≤,a‚Ä≤) ‚àí Q(s,a) ]\n","      </pre>\n","    </li>\n","    <li>Repeat until the agent learns a stable, safe navigation strategy.</li>\n","  </ol>\n","  <p>\n","    Because it is on-policy, SARSA reflects the cautious nature of the policy actually in use, \n","    often preferring safer paths over potentially risky shortcuts.\n","  </p>\n","</div>\n"]},{"cell_type":"code","execution_count":5,"id":"323d4e5a","metadata":{"execution":{"iopub.execute_input":"2025-09-23T14:03:03.30136Z","iopub.status.busy":"2025-09-23T14:03:03.301014Z","iopub.status.idle":"2025-09-23T14:03:03.336333Z","shell.execute_reply":"2025-09-23T14:03:03.335184Z"},"papermill":{"duration":0.040859,"end_time":"2025-09-23T14:03:03.33787","exception":false,"start_time":"2025-09-23T14:03:03.297011","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Training complete! Final Q-table shape: (36, 4)\n"]}],"source":["# =========================================================\n","# Step 4 ‚Äî SARSA Implementation\n","# =========================================================\n","\n","def train_sarsa(env, episodes=500, alpha=0.1, gamma=0.99, epsilon=0.1):\n","    # Initialize Q-table\n","    Q = np.zeros((env.rows * env.cols, len(ACTIONS)))\n","    episode_rewards = []\n","\n","    for ep in range(episodes):\n","        state = env.reset()\n","        s_idx = state[0] * env.cols + state[1]\n","        action_idx = epsilon_greedy(Q, s_idx, epsilon)\n","        total_reward = 0\n","        done = False\n","\n","        while not done:\n","            action = ACTIONS[action_idx]\n","            new_state, reward, done = env.step(action)\n","            ns_idx = new_state[0] * env.cols + new_state[1]\n","\n","            # Choose next action ON-POLICY\n","            next_action_idx = epsilon_greedy(Q, ns_idx, epsilon)\n","\n","            # SARSA update\n","            Q[s_idx, action_idx] += alpha * (reward + gamma * Q[ns_idx, next_action_idx] - Q[s_idx, action_idx])\n","\n","            s_idx = ns_idx\n","            action_idx = next_action_idx\n","            total_reward += reward\n","\n","        episode_rewards.append(total_reward)\n","\n","    return Q, episode_rewards\n","\n","# --- Train SARSA agent ---\n","Q_sarsa, sarsa_rewards = train_sarsa(env, episodes=300, alpha=0.1, gamma=0.95, epsilon=0.2)\n","print(\"Training complete! Final Q-table shape:\", Q_sarsa.shape)\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"papermill":{"default_parameters":{},"duration":5.983784,"end_time":"2025-09-23T14:03:03.759597","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-09-23T14:02:57.775813","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}