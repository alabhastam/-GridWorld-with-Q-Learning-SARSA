{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aabdollahii/gridworld-with-q-learning-sarsa?scriptVersionId=263398440\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<div style=\"background-color:#0d1117; color:#c9d1d9; padding:20px; border-radius:8px; font-family:Segoe UI, sans-serif; line-height:1.6;\">\n  <h1 style=\"color:#58a6ff;\">üèÅ GridWorld: Q-Learning vs SARSA</h1>\n  \n  <p>\n    In this project, we explore two cornerstone algorithms of <strong>Classic Reinforcement Learning</strong>:\n    <span style=\"color:#d2a8ff;\">Q-Learning</span> and <span style=\"color:#d2a8ff;\">SARSA</span>.\n    Using a custom-built <strong>GridWorld</strong> environment, we train an agent to navigate from a start\n    position to a goal while avoiding traps and obstacles.\n  </p>\n\n  <h2 style=\"color:#58a6ff;\">üîç Why GridWorld?</h2>\n  <p>\n    GridWorld provides a clean, interpretable environment where we can visualize learning policies \n    and compare algorithm behaviors side-by-side. Adding traps and movement penalties \n    forces each algorithm to balance risk and reward ‚Äî a great setup to see their differences in action.\n  </p>\n\n  <h2 style=\"color:#58a6ff;\">üìà Project Goals</h2>\n  <ul>\n    <li>Implement a minimal but functional GridWorld without external dependencies.</li>\n    <li>Train agents using <strong>Q-Learning</strong> (off-policy) and <strong>SARSA</strong> (on-policy).</li>\n    <li>Track and compare learning curves, convergence speeds, and final policies.</li>\n    <li>Visualize results in a dark-themed, publication-ready format.</li>\n  </ul>\n\n  <h2 style=\"color:#58a6ff;\">üß† Key Difference</h2>\n  <p>\n    <strong>Q-Learning</strong> always updates toward the <em>best possible action</em> in the next state, \n    even if the agent doesn't take it.<br>\n    <strong>SARSA</strong> updates based on the <em>actual action</em> taken in the next state.\n    This subtle change can lead to dramatically different navigation styles.\n  </p>\n\n  <p style=\"margin-top:15px; font-style:italic; color:#8b949e;\">\n    By the end of this notebook, you'll see how two seemingly similar algorithms \n    can make very different choices in the same world.\n  </p>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color:#0d1117; color:#c9d1d9; padding:18px; border-radius:8px; font-family:Segoe UI, sans-serif; line-height:1.6;\">\n  <h2 style=\"color:#58a6ff;\">üõ† Step 1 ‚Äî Setup & Imports</h2>\n  <p>\n    In this step, we prepare the core components needed for our <strong>GridWorld</strong> project:\n  </p>\n  <ul>\n    <li>Importing <code>numpy</code> and <code>matplotlib</code> for numerical operations and visualizations.</li>\n    <li>Configuring <strong>dark-themed plots</strong> for a cleaner Kaggle output.</li>\n    <li>Setting a <strong>random seed</strong> for reproducibility across runs.</li>\n    <li>Defining the <strong>environment grid size</strong> and available <strong>agent actions</strong>.</li>\n  </ul>\n  <p>\n    This minimal setup ensures our environment is ready for defining GridWorld mechanics \n    and plugging in <span style=\"color:#d2a8ff;\">Q-Learning</span> and <span style=\"color:#d2a8ff;\">SARSA</span> algorithms in the next steps.\n  </p>\n</div>\n","metadata":{}},{"cell_type":"code","source":"\n# Step 1 ‚Äî Setup & Imports\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n# Optional: make plots dark-themed by default for Kaggle\nplt.style.use('dark_background')\n\n# For reproducibility\nRANDOM_SEED = 42\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\n\n# Environment size\nGRID_ROWS = 6\nGRID_COLS = 6\n\n# Actions (Up, Right, Down, Left)\nACTIONS = ['U', 'R', 'D', 'L']\nACTION_IDX = {a: i for i, a in enumerate(ACTIONS)}\n\nprint(f\"Environment: {GRID_ROWS}x{GRID_COLS} | Actions: {ACTIONS}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T15:01:55.257671Z","iopub.execute_input":"2025-09-22T15:01:55.258036Z","iopub.status.idle":"2025-09-22T15:01:55.270911Z","shell.execute_reply.started":"2025-09-22T15:01:55.258009Z","shell.execute_reply":"2025-09-22T15:01:55.269983Z"}},"outputs":[{"name":"stdout","text":"Environment: 6x6 | Actions: ['U', 'R', 'D', 'L']\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"<div style=\"background-color:#0d1117; color:#c9d1d9; padding:18px; border-radius:8px; font-family:Segoe UI, sans-serif; line-height:1.6;\">\n  <h2 style=\"color:#58a6ff;\">üåç Step 2 ‚Äî Building the GridWorld Environment</h2>\n  <p>\n    Here we define our custom <strong>GridWorld</strong> ‚Äî a rectangular grid where an agent\n    navigates from a <span style=\"color:#d2a8ff;\">start position</span> to a <span style=\"color:#d2a8ff;\">goal</span>,\n    avoiding obstacles and traps along the way.\n  </p>\n  <h3 style=\"color:#58a6ff;\">Key Features</h3>\n  <ul>\n    <li>Supports <strong>obstacles (#)</strong> that block movement.</li>\n    <li>Includes <strong>traps (X)</strong> that end the episode with a penalty.</li>\n    <li>Grants a reward of <code>+10</code> for reaching the goal (<strong>G</strong>).</li>\n    <li>Applies a penalty of <code>-5</code> for traps, and <code>-1</code> for each step to encourage efficiency.</li>\n    <li>Maintains agent‚Äôs state as a <code>(row, col)</code> coordinate.</li>\n  </ul>\n  <p>\n    The <code>step()</code> method handles movement, boundaries, obstacles, rewards, and terminal states.\n    The <code>render()</code> method lets us <strong>visualize the grid</strong> and monitor the agent‚Äôs position.\n  </p>\n</div>\n","metadata":{}},{"cell_type":"code","source":"# Step 2 ‚Äî Define GridWorld Environment\n\nclass GridWorld:\n    def __init__(self, rows, cols, start, goal, obstacles=None, traps=None):\n        self.rows = rows\n        self.cols = cols\n        self.start = start\n        self.goal = goal\n        self.obstacles = obstacles if obstacles else []\n        self.traps = traps if traps else []\n        self.reset()\n    \n    def reset(self):\n        \"\"\"Reset the agent to the start position.\"\"\"\n        self.agent_pos = self.start\n        return self.state()\n    \n    def state(self):\n        \"\"\"Return current state as (row, col) tuple.\"\"\"\n        return self.agent_pos\n    \n    def step(self, action):\n        \"\"\"Take an action and return new_state, reward, done.\"\"\"\n        r, c = self.agent_pos\n        if action == 'U': r -= 1\n        elif action == 'D': r += 1\n        elif action == 'L': c -= 1\n        elif action == 'R': c += 1\n        \n        # Stay inside bounds\n        if r < 0: r = 0\n        if r >= self.rows: r = self.rows - 1\n        if c < 0: c = 0\n        if c >= self.cols: c = self.cols - 1\n        \n        # If next cell is obstacle ‚Üí stay put\n        if (r, c) in self.obstacles:\n            r, c = self.agent_pos\n        \n        self.agent_pos = (r, c)\n        \n        # Determine reward and done\n        if self.agent_pos == self.goal:\n            return self.state(), 10, True  # Goal reward\n        elif self.agent_pos in self.traps:\n            return self.state(), -5, True  # Trap penalty\n        else:\n            return self.state(), -1, False  # Step cost\n    \n    def render(self):\n        \"\"\"Print the current grid with agent position.\"\"\"\n        grid = [['.' for _ in range(self.cols)] for _ in range(self.rows)]\n        for (ro, co) in self.obstacles:\n            grid[ro][co] = '#'\n        for (rt, ct) in self.traps:\n            grid[rt][ct] = 'X'\n        gr, gc = self.goal\n        grid[gr][gc] = 'G'\n        ar, ac = self.agent_pos\n        grid[ar][ac] = 'A'\n        \n        for row in grid:\n            print(' '.join(row))\n        print()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T15:01:55.289092Z","iopub.execute_input":"2025-09-22T15:01:55.289521Z","iopub.status.idle":"2025-09-22T15:01:55.306729Z","shell.execute_reply.started":"2025-09-22T15:01:55.289485Z","shell.execute_reply":"2025-09-22T15:01:55.305487Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Create a 6x6 GridWorld example\nenv = GridWorld(\n    rows=GRID_ROWS,\n    cols=GRID_COLS,\n    start=(0, 0),\n    goal=(5, 5),\n    obstacles=[(1, 1), (2, 3), (3, 3), (4, 1)],\n    traps=[(1, 4), (4, 4)]\n)\n\nstate = env.reset()\nenv.render()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T15:01:55.308181Z","iopub.execute_input":"2025-09-22T15:01:55.308707Z","iopub.status.idle":"2025-09-22T15:01:55.338239Z","shell.execute_reply.started":"2025-09-22T15:01:55.308668Z","shell.execute_reply":"2025-09-22T15:01:55.337179Z"}},"outputs":[{"name":"stdout","text":"A . . . . .\n. # . . X .\n. . . # . .\n. . . # . .\n. # . . X .\n. . . . . G\n\n","output_type":"stream"}],"execution_count":3}]}