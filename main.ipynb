{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"background-color:#0d1117; color:#c9d1d9; padding:20px; border-radius:8px; font-family:Segoe UI, sans-serif; line-height:1.6;\">\n  <h1 style=\"color:#58a6ff;\">üèÅ GridWorld: Q-Learning vs SARSA</h1>\n  \n  <p>\n    In this project, we explore two cornerstone algorithms of <strong>Classic Reinforcement Learning</strong>:\n    <span style=\"color:#d2a8ff;\">Q-Learning</span> and <span style=\"color:#d2a8ff;\">SARSA</span>.\n    Using a custom-built <strong>GridWorld</strong> environment, we train an agent to navigate from a start\n    position to a goal while avoiding traps and obstacles.\n  </p>\n\n  <h2 style=\"color:#58a6ff;\">üîç Why GridWorld?</h2>\n  <p>\n    GridWorld provides a clean, interpretable environment where we can visualize learning policies \n    and compare algorithm behaviors side-by-side. Adding traps and movement penalties \n    forces each algorithm to balance risk and reward ‚Äî a great setup to see their differences in action.\n  </p>\n\n  <h2 style=\"color:#58a6ff;\">üìà Project Goals</h2>\n  <ul>\n    <li>Implement a minimal but functional GridWorld without external dependencies.</li>\n    <li>Train agents using <strong>Q-Learning</strong> (off-policy) and <strong>SARSA</strong> (on-policy).</li>\n    <li>Track and compare learning curves, convergence speeds, and final policies.</li>\n    <li>Visualize results in a dark-themed, publication-ready format.</li>\n  </ul>\n\n  <h2 style=\"color:#58a6ff;\">üß† Key Difference</h2>\n  <p>\n    <strong>Q-Learning</strong> always updates toward the <em>best possible action</em> in the next state, \n    even if the agent doesn't take it.<br>\n    <strong>SARSA</strong> updates based on the <em>actual action</em> taken in the next state.\n    This subtle change can lead to dramatically different navigation styles.\n  </p>\n\n  <p style=\"margin-top:15px; font-style:italic; color:#8b949e;\">\n    By the end of this notebook, you'll see how two seemingly similar algorithms \n    can make very different choices in the same world.\n  </p>\n</div>\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}